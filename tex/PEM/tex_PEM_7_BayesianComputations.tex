%%%%%%%%%%%%%%%%%%%%%%%%%
% BAYESIAN COMPUTATIONS %
%%%%%%%%%%%%%%%%%%%%%%%%%
Generally Bayesian posteriors feature an analytic closed-form expression only on a rare occasion.
Specifically this applies to posteriors of the form \cref{eq:PEM:Multilevel:Inference:JointPosterior,eq:PEM:Multilevel:Marginal:Posterior,eq:PEM:Multilevel:PerfectData:Posterior}.
% MARKOV CHAIN MONTE CARLO
Notwithstanding the above, posteriors can be explored by means of Markov chain Monte Carlo (MCMC) \cite{MCMC:Gilks1996,MCMC:Robert2004}.
Principally this readily refers to posteriors stemming from multilevel inversion.
% MH & GIBBS SAMPLING
The Metropolis-Hastings (MH) algorithm and the Gibbs sampler are prototypical MCMC techniques.
% SECTION OUTLINE
In \cref{sec:PEM:Computations:MH} we will review the MH algorithm and discuss classical MCMC key issues in \cref{sec:PEM:Computations:KeyChallenges}.
Additional computational key challenges posed by Bayesian multilevel model calibration will be discussed in \cref{sec:PEM:Computations:MultilevelChallenges}.
Some more sophisticated MCMC samplers that are suitable in a multilevel-context are surveyed in \cref{sec:PEM:Computations:AdvancedSamplers}.

\subsection{The Metropolis-Hastings algorithm} \label{sec:PEM:Computations:MH}
% MCMC PRINCIPLE
MCMC is based on constructing an ergodic Markov chain such that its invariant distribution equals the posterior.
% METROPOLIS-HASTINGS
Let \(\pi(\bm{q})\) be the prior and \(\pi(\bm{q} \cond \tuple{\bm{y}_i})\) the posterior density of some QoI \(\bm{q}\).
A Markov chain with equilibrium distribution \(\pi(\bm{q} \cond \tuple{\bm{y}_i})\) is generated by initializing at \(\bm{q}^{(0)}\) and repetitively proceeding as follows.
Given a state \(\bm{q}^{(t)}\) that the Markov chain has taken on in some iteration, in the following iteration a candidate state
\(\bm{q}^{(\star)} \sim P(\bm{q}^{(\star)} \cond \bm{q}^{(t)})\) is randomly sampled from a proposal distribution \(P(\bm{q}^{(\star)} \cond \bm{q}^{(t)})\).
In the MH correction step the proposed state is approved as the new state \(\bm{q}^{(t+1)} = \bm{q}^{(\star)}\) of the Markov chain with probability
\begin{equation} \label{eq:PEM:MCMC:MHCorrection}
  \alpha \left( \bm{q}^{(\star)}, \bm{q}^{(t)} \right)
  \, = \, \mathrm{min} \left( 1,\frac{ \pi(\bm{q}^{(\star)} \cond \tuple{\bm{y}_i}) \, P(\bm{q}^{(t)} \cond \bm{q}^{(\star)}) }
                                     { \pi(\bm{q}^{(t)} \cond \tuple{\bm{y}_i}) \, P(\bm{q}^{(\star)} \cond \bm{q}^{(t)})     } \right).
\end{equation}
Otherwise the proposal will be rejected, i.e.\ the Markov chain remains in its state \(\bm{q}^{(t+1)} = \bm{q}^{(t)}\) of the preceding iteration.
% UNSCALED POSTERIOR
It is important to note that due to the MH acceptance probability \cref{eq:PEM:MCMC:MHCorrection}, the algorithm calls for the computation of posterior ratios only.
Thus for MCMC sampling the scale factors in \cref{eq:PEM:Multilevel:Inference:JointPosterior,eq:PEM:Multilevel:Marginal:Posterior,eq:PEM:Multilevel:PerfectData:Posterior}
can be dropped and only unscaled posterior densities have to be evaluated.
\par % SAMPLING SCHEMES
% RANDOM WALK SAMPLING
Random walk Metropolis sampling rests upon local proposals, e.g.\ candidate states are sampled from a Gaussian distribution
\(\bm{q}^{(\star)} \sim \mathcal{N}(\bm{q}^{(\star)} \distparam \bm{q}^{(t)},\bm{\Sigma}_{\bm{q}})\) that is centered around the current state \(\bm{q}^{(t)}\).
The covariance matrix \(\bm{\Sigma}_{\bm{q}}\) determines the ``stepsizes'' of the algorithm.
% INDEPENDENCE SAMPLING
Independence MH sampling is based on nonlocal proposals whose distribution \(\bm{q}^{(\star)} \sim P(\bm{q}^{(\star)})\) is independent of \(\bm{q}^{(t)}\),
e.g.\ sampling candidate states from the prior \(\bm{q}^{(\star)} \sim \pi(\bm{q}^{(\star)})\) or from some suitable approximation of the posterior \(\bm{q}^{(\star)} \sim \hat{\pi}(\bm{q}^{(\star)} \cond \tuple{\bm{y}_i})\).

\subsection{Classical key challenges} \label{sec:PEM:Computations:KeyChallenges}
% MIXING PROPERTIES
The performance of MCMC methods is governed by the mixing properties of the underlying Markov chain, i.e.\ the speed of convergence of the Markov chain towards the targeted posterior.
% AUTOCORRElATION
As to which degree MCMC samples are autocorrelated has a determining influence on the convergence speed and their quality as posterior representatives.
% DESIGN & TUNING
Hence MCMC algorithms are designed and tuned in pursuit of rapid mixing.
Depending on the specific problem at hand, this may be a tricky business which requires to employ and combine sophisticated and highly specialized sampling schemes.
% FORWARD MODEL RUNS
Typically MCMC sampling calls for a high number of program iterations which in turn demands a high number of forward model runs for evaluating the likelihood function in the MH correction \cref{eq:PEM:MCMC:MHCorrection}.
% CONVERGENCE CHECKS
Beyond that, careful convergence diagnostics are of particular importance for MCMC methods.
One has to assess when the Markov chain has reached its stationary distribution, i.e.\ when it has lost any dependence on its initialization.
% ADVANCED DIAGNOSTICS & HEURISTICS
Even though there are advanced convergence test \cite{MCMC:Cowles1996,MCMC:Brooks1998:Roberts},
e.g.\ Gelman-Rubin diagnostics for multiple over-dispersed chains \cite{MCMC:Gelman1992,MCMC:Brooks1998:Gelman},
we remark that from a pessimistic point of view any convergence diagnostic is heuristics \cite{MCMC:Geyer2011:a}.
% HIGH-DIMENSION & MULTIMODALITY
Furthermore MCMC suffers from difficulties in exploring high-dimensional and multimodal posteriors.

\subsection{Multilevel-related challenges} \label{sec:PEM:Computations:MultilevelChallenges}
% MULTILEVEL MCMC
Multilevel posteriors can be readily sampled by means of classical MCMC techniques as they are commonly applied in ``simple'' Bayesian inversion.
However, on top of the classical bottlenecks that were discussed above, one is faced with multilevel-specific MCMC challenges.
% JOINT vs MARGINAL
The posteriors \cref{eq:PEM:Multilevel:Inference:JointPosterior,eq:PEM:Multilevel:Marginal:Posterior}, which are appertain to the joint and the marginal variant of multilevel calibration, are different in nature.
Accordingly, sampling these posteriors pose different computational burdens.
The former requires a sampling scheme that performs efficiently in high-dimensional parameter spaces, whereas the latter suffers from computing the integrated likelihood \cref{eq:PEM:Multilevel:Marginal:Likelihood}.
% PERFECT DATA
Similarly the posterior \cref{eq:PEM:Multilevel:PerfectData:Posterior} of the ``perfect'' data model imposes forward uncertainty quantification for the computation of the likelihood \cref{eq:PEM:Multilevel:PerfectData:Likelihood}.
\par % LIKELIHOOD ESTIMATION
Likelihood functions of the form \cref{eq:PEM:Multilevel:Marginal:LikelihoodEstimator,eq:PEM:Multilevel:PerfectData:LikelihoodEstimator} suffer from another severe difficulty.
% POSTERIOR FIDELITY
It is well-known that statistical estimations of the likelihood ratio introduce an additional random component into the Markov chain transition kernel \cite{MCMC:ONeill2000,MCMC:Bal2013}.
Consequently the steady-state distribution of the chain may be modified.
Therefore free parameters of the algorithm have to be chosen endeavoring high \textit{posterior fidelity},
i.e.\ the degree as to which the induced long-run distribution conforms with the true posterior \cite{Nagel:SciTech2014:Proc,Nagel:JAIS2015}.

\subsection{Advanced MCMC samplers} \label{sec:PEM:Computations:AdvancedSamplers}
% TOTAL NUMBER OF FORWARD MODEL RUNS
Summarized Bayesian multilevel model calibration requires an enormous number of forward model runs.
Therefore in the statistical literature a wide range of advanced MCMC techniques, dedicated to posterior exploration in classical hierarchical models, have been devised.
Some enhanced Gibbs sampling methods in this context are reviewed in \cite{MCMC:Gilks1996} and references therein.
% ENGINEERING APPLICATIONS
However, in view of engineering problems they may not meet the challenges those applications usually pose.
This is due to the inescapable ``blackbox'' character of the forward solver and nonconjugacy.
Generally not all of the parameters will have full conditionals of a standard form that can be easily sampled.
% DA & HMC
Despite that this paper does not focus on computational facets of uncertainty quantification, a short outlook on potentially efficient MCMC implementations is given.
\par % DATA AUGMENTIATON
Data augmentation is a powerful MCMC technique that aims at enhancing the numerical efficiency of posterior computation by introducing missing data as auxiliary variables \cite{MCMC:Dyk2001,MCMC:Dyk2003}.
% NATURALL EMERGENCE
Note that the joint posterior \cref{eq:PEM:Multilevel:Inference:JointPosterior} can be seen as an augmented form of the marginal one in \cref{eq:PEM:Multilevel:Marginal:Posterior}.
Thus data augmentation naturally emerges in the context of Bayesian multilevel inversion.
% PARTIAL DATA AUGMENTATION
It has been beneficially applied for solving multilevel inverse problems within the domain of aerospace engineering \cite{Nagel:SciTech2014:Proc,Nagel:JAIS2015}.
% PSEUDO MARGINALIZATION
Vice versa, there are dedicated MCMC schemes for directly computing marginalized posteriors of the form \cref{eq:PEM:Multilevel:Marginal:Posterior},
e.g.\ MC within Metropolis sampling \cite{MCMC:ONeill2000,MCMC:Beaumont2003} or pseudo-marginalization \cite{MCMC:Andrieu2009}.
% HAMILTONIAN MONTE CARLO
The Hamiltonian Monte Carlo (HMC) algorithm is a sampler whose performance is remarkably efficient in high-dimensional parameter spaces and for highly correlated posteriors \cite{MCMC:Duane1987,MCMC:Neal2011}.
Since multilevel models are higher-dimensional and correlated by definition, the HMC is a promising MCMC candidate in this context.
% (UNDER)ACKNOWLEDGEMENT
Yet the HMC still occurs to be highly underacknowledged in Bayesian inference in general and for hierarchical models in particular.